import torch
import os

from cvpods.engine.runner import DefaultRunner
from cvpods.engine import hooks
from cvpods.modeling.nn_utils.precise_bn import get_bn_modules
from cvpods.utils import comm
from cvpods.engine.base_runner import RUNNERS


@RUNNERS.register()
class SWAVRunner(DefaultRunner):
    def __init__(self, cfg, model_builder):
        super(SWAVRunner, self).__init__(cfg, model_builder)

    def resume_or_load(self, resume=True):
        super().resume_or_load(resume)
        if comm.get_world_size() > 1:
            self.model.module.steps = self.start_iter
            self.model.module.epoch = self.start_epoch
        else:
            self.model.steps = self.start_iter
            self.model.epoch = self.start_epoch

    def build_hooks(self):
        """
        Build a list of default hooks, including timing, evaluation,
        checkpointing, lr scheduling, precise BN, writing events.

        Returns:
            list[HookBase]:
        """
        cfg = self.cfg
        # cfg.DATALOADER.NUM_WORKERS = 0  # save some memory and time for PreciseBN

        ret = [
            SwavOptimizationHook(
                accumulate_grad_steps=cfg.SOLVER.BATCH_SUBDIVISIONS,
                grad_clipper=None,
                mixed_precision=cfg.TRAINER.FP16.ENABLED,
                cancel_epochs=cfg.MODEL.SWAV.CANCEL_EPOCHS,
            ),
            hooks.LRScheduler(self.optimizer, self.scheduler),
            hooks.IterationTimer(),
            hooks.PreciseBN(
                # Run at the same freq as (but before) evaluation.
                cfg.TEST.EVAL_PERIOD,
                self.model,
                # Build a new data loader to not affect training
                self.build_train_loader(cfg),
                cfg.TEST.PRECISE_BN.NUM_ITER,
            )
            if cfg.TEST.PRECISE_BN.ENABLED and get_bn_modules(self.model)
            else None,
        ]

        # Do PreciseBN before checkpointer, because it updates the model and need to
        # be saved by checkpointer.
        # This is not always the best: if checkpointing has a different frequency,
        # some checkpoints may have more precise statistics than others.
        if comm.is_main_process():
            ret.append(hooks.PeriodicCheckpointer(
                self.checkpointer,
                cfg.SOLVER.CHECKPOINT_PERIOD,
                max_iter=self.max_iter,
                max_epoch=self.max_epoch
            ))

        def test_and_save_results():
            self._last_eval_results = self.test(self.cfg, self.model)
            return self._last_eval_results

        # Do evaluation after checkpointer, because then if it fails,
        # we can use the saved checkpoint to debug.
        ret.append(hooks.EvalHook(cfg.TEST.EVAL_PERIOD, test_and_save_results))

        if comm.is_main_process():
            # Here the default print/log frequency of each writer is used.
            # run writers in the end, so that evaluation metrics are written
            ret.append(hooks.PeriodicWriter(
                self.build_writers(),
                period=self.window_size))
            # Put `PeriodicDumpLog` after writers so that can dump all the files,
            # including the files generated by writers
            if cfg.OSS.DUMP_LOG_ENABLED:
                if cfg.OSS.DUMP_PERIOD == 0:
                    dump_log_period = cfg.SOLVER.CHECKPOINT_PERIOD
                else:
                    dump_log_period = cfg.OSS.DUMP_PERIOD
                ret.append(hooks.PeriodicDumpLog(
                    cfg.OUTPUT_DIR, cfg.OSS.DUMP_PREFIX, dump_log_period))
        return ret


class SwavOptimizationHook(hooks.HookBase):
    def __init__(self, accumulate_grad_steps=1, grad_clipper=None, mixed_precision=False, cancel_epochs=1):
        self.accumulate_grad_steps = accumulate_grad_steps
        self.grad_clipper = grad_clipper
        self.mixed_precision = mixed_precision
        self.cancel_epochs = cancel_epochs

    def before_step(self):
        if self.trainer.iter % self.accumulate_grad_steps == 0:
            self.trainer.optimizer.zero_grad()

    def after_step(self):
        losses = self.trainer.step_outputs["loss_for_backward"]
        losses /= self.accumulate_grad_steps

        if self.mixed_precision:
            from apex import amp
            with amp.scale_loss(losses, self.trainer.optimizer) as scaled_loss:
                scaled_loss.backward()
        else:
            if self.trainer.iter % self.accumulate_grad_steps != 0:
                with self.trainer.model.no_sync():
                    losses.backward()
            else:
                losses.backward()

        # cancel some gradients: ONLY used for SwAV
        if self.trainer.iter < self.trainer.epoch_iters * self.cancel_epochs:
            for name, p in self.trainer.model.named_parameters():
                if "prototypes" in name:
                    p.grad = None

        if self.trainer.iter % self.accumulate_grad_steps == 0:
            # if self.grad_clipper is not None:
            #     self.grad_clipper(self.tariner.model.paramters())
            self.trainer.optimizer.step()
            self.trainer.optimizer.zero_grad()
